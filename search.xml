<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Parallel Computing]]></title>
    <url>%2F2019%2F04%2F23%2Fparallel-computing%2F</url>
    <content type="text"><![CDATA[MPI tutorial]]></content>
      <tags>
        <tag>MPI</tag>
        <tag>openmp</tag>
        <tag>Parallel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CUDA C Programming notes]]></title>
    <url>%2F2019%2F04%2F23%2Fcuda-c-programming-notes%2F</url>
    <content type="text"><![CDATA[TODO使用cudaStream实现计算和内存拷贝重叠简要：并不是每个Stream对应一个queue，然后两个queue就会寻找重叠的机会；时刻意识到实际的模型是一个CopyEngine（新的结构可能支持俩个，一个to一个from），一个Kernel Engine。Engine是按照将操作加入Stream的顺序来调度的。策略可以概括为将操作按照不同streams广度优先（breadth-first）顺序排列。CUDA by Example, Chapter 10. Jason Sanders, Edward Kandrot. MPI CUDA Mixed ProgrammingCONCURRENT KERNELSCONCURRENT KERNELS II: BATCHED LIBRARY CALLSParallel CUDAnice notes]]></content>
      <tags>
        <tag>CUDA</tag>
        <tag>tutorial</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Doxygen Getting Started]]></title>
    <url>%2F2019%2F04%2F19%2Fdoxygen-notes%2F</url>
    <content type="text"><![CDATA[1alert('Hello World!'); 简单上手1.安装$brew install doxygen 2.生成配置文件$doxygen -g [filename] 3.运行$doxygen [configfile]默认会生成html和latex 注释类或者函数之前12345/** \brief some brief description* brief description continued** Detailed description*/ 一般函数或者成员变量12/// get name of nodestd::string name() const; 1.希望有brief一行12/// get name of nodestd::string name() const; 2.没有brief，有Detailed description两行以上123/// get name of node/// \return std::stringstd::string name() const; 3.希望既有brief，又有detailed description隔开的///1234/// brief description/// \return std::stringstd::string name() const; 多行注释1234567/*** \brief brief description** detailed desc start * here */std::string name() const; 单行注释加多行注释123456/// brief description/** detailed description start* here, like *\return std::string*/std::string name() const;]]></content>
      <tags>
        <tag>doxygen</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SWCompiler开发笔记]]></title>
    <url>%2F2019%2F04%2F01%2Fswcompiler-dev-notes%2F</url>
    <content type="text"><![CDATA[关于导入模型时命名的问题对于lenet mnist的例子，网络比较小，简单地以每个op的output作为node的name，以及name+”_out”作为其输出Tensor没有出问题。然而，对于比价复杂的Resnet50网络，则带来了如下的问题。1234567891011121314151617181920212223242526272829303132333435363738394041424344op &#123; input: "gpu_0/res3_2_branch2c" input: "gpu_0/res3_2_branch2c_bn_s" input: "gpu_0/res3_2_branch2c_bn_b" input: "gpu_0/res3_2_branch2c_bn_rm" input: "gpu_0/res3_2_branch2c_bn_riv" output: "gpu_0/res3_2_branch2c_bn" name: "" type: "SpatialBN" arg &#123; name: "epsilon" f: 1.00000006569e-05 &#125; arg &#123; name: "is_test" i: 1 &#125; arg &#123; name: "order" s: "NCHW" &#125; device_option &#123; &#125; engine: ""&#125;op &#123; input: "gpu_0/res3_2_branch2c_bn" input: "gpu_0/res3_1_branch2c_bn" output: "gpu_0/res3_2_branch2c_bn" name: "" type: "Sum" device_option &#123; &#125; engine: ""&#125;op &#123; input: "gpu_0/res3_2_branch2c_bn" output: "gpu_0/res3_2_branch2c_bn" name: "" type: "Relu" device_option &#123; &#125; engine: ""&#125; 以Sum节点为例，其输出是输入之一。input(0)和output(0)对应的不同的TensorNode（且名字也应该不同，因为dotGen时要用到）;但这俩个TensorNode最好指向同一个Tensor（否则还要做优化，比如这里Sum操作的原义是将B加到A上，而最好不要C=A+B）。 以Relu节点为例，其输入gpu_0/res3_2_branch2c_bn是Sum的输出，也是BN的输出。但是其在拓扑上应该与Relu节点连接。 为了避免极可能的数组访问越界，摒弃直接index访问元素，转而封装成函数，内部可以报错或者是创建不存在的节点。 去除tempalte时一系列duplicate symbol错误12345678910111213141516171819[100%] Linking CXX executable testMLPduplicate symbol __ZNK3swc10TensorNode8toStringEv in: CMakeFiles/testMLP.dir/test/testMLP.cpp.o libgraphcore.a(IRGraph.cpp.o)duplicate symbol __ZNK3swc6OpNode8toStringEv in: CMakeFiles/testMLP.dir/test/testMLP.cpp.o libgraphcore.a(IRGraph.cpp.o)duplicate symbol __ZNK3swc10TensorNode5cloneEv in: CMakeFiles/testMLP.dir/test/testMLP.cpp.o libgraphcore.a(IRGraph.cpp.o)duplicate symbol __ZNK3swc6OpNode5cloneEv in: CMakeFiles/testMLP.dir/test/testMLP.cpp.o libgraphcore.a(IRGraph.cpp.o)duplicate symbol __ZTVN3swc10TensorNodeE in: CMakeFiles/testMLP.dir/test/testMLP.cpp.o libgraphcore.a(IRGraph.cpp.o)duplicate symbol __ZTSN3swc10TensorNodeE in: CMakeFiles/testMLP.dir/test/testMLP.cpp.o libgraphcore.a(IRGraph.cpp.o) 开始莫名其妙，细看后意识到时特定的函数，它们定义在类之外。即std::string TensorNode::toStringEv() const {} 在使用模板类时，这些成员函数定义在.h文件是没有问题的，因为在.cpp中才是真正的实例化。 去掉模板后，这些定义在类结构外的成员函数，在头文件被多个其他文件包含时，函数定义也会include进去，从而造成duplica问题。解决办法：要么将所有成员函数定义在类结构体中，要么定义在.cpp中。]]></content>
  </entry>
  <entry>
    <title><![CDATA[CMake Tutorial]]></title>
    <url>%2F2019%2F03%2F25%2Fcmake-tutorial%2F</url>
    <content type="text"><![CDATA[add_custom_command例如，Glow中在编译时运行编译得到的InstrGen和NodeGen生成对应的源代码文件，用于接下来的编译。123456789101112# AutoGenNodesset(NODES_HDR $&#123;GLOW_BINARY_DIR&#125;/glow/AutoGenNodes.h)set(NODES_SRC $&#123;GLOW_BINARY_DIR&#125;/glow/AutoGenNodes.cpp)set(NODES_DEF $&#123;GLOW_BINARY_DIR&#125;/glow/AutoGenNodes.def)add_custom_command(OUTPUT "$&#123;NODES_HDR&#125;" "$&#123;NODES_SRC&#125;" "$&#123;NODES_DEF&#125;" COMMAND NodeGen $&#123;NODES_HDR&#125; $&#123;NODES_SRC&#125; $&#123;NODES_DEF&#125; DEPENDS NodeGen COMMENT "NodeGen: Generating nodes." VERBATIM) 在文件改变时拷贝文件到指定目录。1234add_custom_command( OUTPUT $&#123;GLOW_BINARY_DIR&#125;/glow/caffe.pb.h COMMAND $&#123;CMAKE_COMMAND&#125; -E copy_if_different $&#123;CAFFE_HDRS&#125; $&#123;GLOW_BINARY_DIR&#125;/glow/caffe.pb.h DEPENDS $&#123;CAFFE_HDRS&#125;) 然而，仅仅这样，命令并不会被执行。除非其OUTPUT时某个target的依赖项。1234567891011add_library(Importer ProtobufLoader.cpp Caffe2.cpp Caffe.cpp ONNX.cpp ONNXIFILoader.cpp ONNXExporter.cpp $&#123;CAFFE_SRCS&#125; $&#123;CAFFE2_SRCS&#125; $&#123;GLOW_BINARY_DIR&#125;/glow/caffe.pb.h $&#123;GLOW_BINARY_DIR&#125;/glow/caffe2.pb.h) 因此一个办法是，作为某个target/library的依赖项；另一办法是专门为某个源文件添加property。set_property(SOURCE Caffe.cpp APPEND PROPERTY OBJECT_DEPENDS ${GLOW_BINARY_DIR}/glow/caffe.pb.h) install 命令参考CMake的installSirDigit的博客 install命令并非在cmake或者make编译时运行，而是在编译后执行install时。以makefile为例，cmake中的install命令会生成到对应的Makefile中，对应make install命令。 CMAKE_INSTALL_PREFIX : 默认安装路径，一般是/usr/local，可以用message命令打印查看。message(STATUS “${CMAKE_INSTALL_PREFIX}”) TARGETS版本的install命令123456789install(TARGETS targets... [EXPORT &lt;export-name&gt;] [[ARCHIVE|LIBRARY|RUNTIME|FRAMEWORK|BUNDLE| PRIVATE_HEADER|PUBLIC_HEADER|RESOURCE] [DESTINATION &lt;dir&gt;] [PERMISSIONS permissions...] [CONFIGURATIONS [Debug|Release|...]] [COMPONENT &lt;component&gt;] [OPTIONAL] [NAMELINK_ONLY|NAMELINK_SKIP] ] [...]) 其中，静态链接库对应ARCHIVE；链接库对应LIBRARY；可执行文件则是RUNTIME。 安装期间要执行脚本或者代码，如打印信息。可以使用CODE或者SCRIPAT版本的install命令,如install(CODE “MESSAGE(\“some message\”)”) cmake 命令时的宏-DMACRONAME=XXX为了实现LEVELDEBUG，即将cmake命令添加-DLEVELDEBUG=5时，当且仅当DEBUG(i) &lt;&lt; 中的i&gt;=5时方进行打印。遇到的问题如下。1.设置LEVELDEBUG宏12cmake -DCMAKE_BUILD_TYPE=Debug .. -DLEVELDEBUG=5vim vim CMakeFiles/testDSL.dir/flags.make 结果CXX_DEFINES = -DLEVELDEBUG=5 2.命令中去除LEVELDEBUG宏12cmake -DCMAKE_BUILD_TYPE=Debug ..vim CMakeFiles/testDSL.dir/flags.make 结果中仍然有该宏。CXX_DEFINES = -DLEVELDEBUG=5另外Release模式，该宏还是存在。 3.设置Release模式时，将该宏设置为极大值100.1234567if(LEVELDEBUG) if(CMAKE_BUILD_TYPE STREQUAL Debug) add_definitions(-DLEVELDEBUG=$&#123;LEVELDEBUG&#125;) else() add_definitions(-DLEVELDEBUG=100) endif()endif () 测试12cmake -DCMAKE_BUILD_TYPE=Release ..vim CMakeFiles/testDSL.dir/flags.make 结果中CXX_DEFINES = -DLEVELDEBUG=100 4.再次切换回Debug模式12cmake -DCMAKE_BUILD_TYPE=Debug ..vim CMakeFiles/testDSL.dir/flags.make 结果仍然恢复为CXX_DEFINES = -DLEVELDEBUG=5这个5可能来自cache中。有待考证。 相关讨论cmake: how to check if preprocessor is defined - Stack Overflow]]></content>
      <tags>
        <tag>cmake</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于CodeGen的一些笔记]]></title>
    <url>%2F2019%2F03%2F10%2Fa-sightseeing-on-codegen%2F</url>
    <content type="text"><![CDATA[TVM TVM的CodeGen仅仅是生成对应的funcion。内存分配则由DeviceAPI负责。以CodeGenCUDA为例，CUDADeviceAPI内部封装了如下功能。 各种DeviceAttr的查询，实现则是直接调用了cudaDeviceGetAttribute()函数。 GPU存储的分配和释放。 数据拷贝。分为GPU到GPU(是否同一device)，CPU到GPU，GPU到CPU等几种类型。TVMContext给出设备信息。1234567typedef struct &#123; /*! \brief The device type used in the device. */ DLDeviceType device_type; /*! \brief The device index */ int device_id;&#125; DLContext;ypedef DLContext TVMContext;]]></content>
  </entry>
  <entry>
    <title><![CDATA[Posts-on-cplusplus]]></title>
    <url>%2F2019%2F03%2F06%2Fposts-on-cplusplus%2F</url>
    <content type="text"><![CDATA[tutorialthispointer.comcppreference.com Concurrencythread | mutex | condition variable | atomicC++11 Concurrency Tutorialuchicago-cs/cmsc12300-Concurrency in C++11packaged_task and threadpackaged_task&lt;&gt; Example and Tutorial : from function, lambda function and function object. 构造函数调用构造函数123456789Tensor(const std::initializer_list&lt;int&gt; &amp;shape)&#123; std::vector&lt;unsigned long&gt; *vec = new std::vector&lt;unsigned long&gt;(); for(auto i : shape)&#123; int v = i; vec-&gt;push_back(v); &#125; // Tensor(new TensorShape(vec)); // 此写法似乎有误 _shape = new TensorShape(vec);&#125;]]></content>
      <tags>
        <tag>C++</tag>
        <tag>tutorial</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Glow源码阅读]]></title>
    <url>%2F2019%2F03%2F04%2Fglow-code-review%2F</url>
    <content type="text"><![CDATA[日志C++ in Glow该部分主要是，结合Glow中的代码示例说明C++一些方法技巧，增强记忆。 虚函数父类使用虚函数。1234// Backend.hvirtual bool transformPostLowering(Function *F, CompilationMode mode) const &#123;return false;&#125; 子类则对方法重载。声明时使用override指示该方法进行了重载。1234567class CPUBackend : public Backend &#123; ... bool transformPostLowering(Function *F, CompilationMode mode) const override;&#125;;bool CPUBackend::transformPostLowering(Function *F, CompilationMode mode) const&#123; ...&#125; 以下来自一篇写得很清晰的文章1) 公有继承纯虚函数 =&gt; 继承的是：接口 (interface)普通虚函数 =&gt; 继承的是：接口 + 缺省实现 (default implementation)非虚成员函数 =&gt; 继承的是：接口 + 强制实现 (mandatory implementation) 对于声明为非虚成员函数，继承时最好不要重写；对于普通虚函数，要重写时需要满足苛刻的条件（返回类型，常量属性，引用限定符等）。使用override关键字，可以强制编译器检查该函数是否正确重写。其中，这三种函数在父类中声明示例如下：123456class Shape &#123;public: virtual void Draw() const = 0; // 1) 纯虚函数 virtual void Error(const string&amp; msg); // 2) 普通虚函数 int ObjectID() const; // 3) 非虚函数&#125;; std::unique_ptr, std::move等先上代码。1234567891011121314151617class OpenCLFunction final : public CompiledFunction &#123; ... std::unique_ptr&lt;IRFunction&gt; F_;public: /// Ctor. explicit OpenCLFunction(std::unique_ptr&lt;IRFunction&gt; F);&#125;;OpenCLFunction::OpenCLFunction(std::unique_ptr&lt;IRFunction&gt; F) : F_(std::move(F)) &#123; ...&#125;std::unique_ptr&lt;CompiledFunction&gt;OCLBackend::compile(std::unique_ptr&lt;IRFunction&gt; IR) const &#123; return llvm::make_unique&lt;OpenCLFunction&gt;(std::move(IR));&#125; unique_ptr 独占所指向的对象, 同一时刻只能有一个 unique_ptr 指向给定对象(通过禁止拷贝语义, 只有移动语义来实现), 定义于 memory (非memory.h)中, 命名空间为 std。 这是一篇很全面的文章。C++ 11 创建和使用 unique_ptr要点是： unique_ptr的构造主要有以下方式。 12std::unique_ptr p = std::make&lt;ClassName&gt;();std::unique_ptr p(new ClassName()); unique_ptr可以实现 返回函数内分配的动态资源 123456789101112unique_ptr&lt;int&gt; Func(int p)&#123; unique_ptr&lt;int&gt; pInt(new int(p)); //构造方式 return pInt; // 返回unique_ptr&#125;int main() &#123; int p = 5; unique_ptr&lt;int&gt; ret = Func(p); cout &lt;&lt; *ret &lt;&lt; endl; // 函数结束后，自动释放资源&#125; 自动释放资源。避免 new delete方式因抛出异常等原因，无法正常释放。 可以使用移动构造和移动赋值，而不能使用复制构造和赋值。’unique_ptr p2(std::move(p1)’或者unique_ptr&lt;T&gt; p2 = std::move(p1); std::moveC++14template&lt; class T &gt;constexpr typename std::remove_reference::type&amp;&amp; move( T&amp;&amp; t ) noexcept;std::move的参数和返回值都是右值引用。在Glow中有例子如下：12345678910void Context::insert(Placeholder *P, Tensor &amp;&amp;T) &#123; assert(!map_.count(P) &amp;&amp; "Placeholder already registered"); // Take ownership over the tensor. map_[P] = new Tensor(std::move(T)); nameMap_[P-&gt;getName()] = P;&#125;// somewhereTensor ptrT = orig-&gt;getUnowned(orig-&gt;dims());insert(placeholders[i], std::move(ptrT)); lambda function回调函数12345678/// Callback type used by HostManager and DeviceManager, used to pass results of an inference request back to the caller.using ResultCBTy = std::function&lt;void( runtime::RunIdentifierTy, runtime::ResultCode, std::unique_ptr&lt;Context&gt;)&gt;;void CPUDeviceManager::runFunctionImpl(RunIdentifierTy id, std::string function, std::unique_ptr&lt;Context&gt; ctx, ResultCBTy resultCB) &#123; ... resultCB(id, ResultCode::Executed, std::move(ctx));&#125; 调用1234567891011121314151617181920/// Starts a run of resnet50 on the given image. The image must be already/// loaded into the input placeholder in /p ctx./// If, at the end of the run the number of \p returned results is equal to/// maxImages, the \p finished promise is set.void dispatchClassify(unsigned int id, DeviceManager *device, std::string path, Placeholder *output, std::unique_ptr&lt;Context&gt; ctx, std::atomic&lt;size_t&gt; &amp;returned, std::promise&lt;void&gt; &amp;finished) &#123; device-&gt;runFunction( "resnet50", std::move(ctx), [id, path, output, &amp;returned, &amp;finished](RunIdentifierTy, ResultCode r, std::unique_ptr&lt;Context&gt; ctx) &#123; size_t maxIdx = ctx-&gt;get(output)-&gt;getHandle&lt;&gt;().minMaxArg().second; llvm::outs() &lt;&lt; "(" &lt;&lt; id &lt;&lt; ") " &lt;&lt; path &lt;&lt; ": " &lt;&lt; maxIdx &lt;&lt; "\n"; if (++returned == maxImages) &#123; finished.set_value(); &#125; &#125;);&#125; lambda expressions in C++ capture clause (Also known as the lambda-introducer in the C++ specification.)在上面的代码示例中，lambda函数体中除了参数，还使用了id，path等来自surrounding scope的变量，即相对于lambda函数的外部变量。capture的参数分为[&amp;](captured-by-reference)和[=](captured-by-value)。by-value是默认方式。假定外部变量total以引用的方式，factor则以值方式，以下方式等价。[&amp;total, factor][&amp;, factor][=, &amp;total]…省略了交换顺序的情况 parameter list Optional. (Also known as the lambda declarator) mutable specification Optional. exception-specification Optional. trailing-return-type Optional. lambda body.另外一个例子。 12345678910111213141516// captures_lambda_expression.cpp// compile with: /W4 /EHsc#include &lt;iostream&gt;using namespace std;int main()&#123; int m = 0; int n = 0; [&amp;, n] (int a) mutable &#123; m = ++n + a; &#125;(4); cout &lt;&lt; m &lt;&lt; endl &lt;&lt; n &lt;&lt; endl;&#125;/** 5* 0*/ Glow 工程架构ExecutionEngine编译流程v1123void ExecutionEngine::compile(CompilationMode mode, Function *F) &#123; function_ = backend_-&gt;compile(generateIR(mode, F));&#125; generateIR()做了大量的工作。 图(class Function)优化 图Lowering (backend 相关的) pre/post-Lowering的图处理 IR(IRFunction)生成 IR 优化 function_的类型是class CompiledFunction，其实是一个接口类。 EE生成IR，并让backend_对IR进一步编译处理； EE在运行图时，调用的是function_-&gt;execute()，该函数是class CompiledFunction的纯虚函数； CPU后端的相关的CPUFunction，OpenCL相关的OpenCLFunction等，都是继承自CompliedFunction。在代码形式上，函数返回类型为Base *， 实际返回则是Derived *。1234567891011class OCLBackend final : public Backend &#123;public: ... std::unique_ptr&lt;CompiledFunction&gt; compile(std::unique_ptr&lt;IRFunction&gt; IR) const override;&#125;;std::unique_ptr&lt;CompiledFunction&gt;OCLBackend::compile(std::unique_ptr&lt;IRFunction&gt; IR) const &#123; return llvm::make_unique&lt;OpenCLFunction&gt;(std::move(IR));&#125; v2 PostOrderVisitorGlow的图遍历采用了visitor模式。ClassName##Node内部均定义了visit()方法；Visitor选择实现pre/post方法来选择先序后序遍历。 Node的visit方法如下,做了简略：12345678910111213141516171819void XXvisit(Node *parent, NodeWalker *visitor)&#123; visitor-&gt;pre(parent, this); for (const auto &amp;op : nodeInputs_) &#123; op.getNode()-&gt;visit(this, visitor); &#125; for (const auto &amp;op : members_) &#123; if (op.first == MemberType::VectorNodeValue) &#123; for (auto &amp;I : &lt;&lt; op.second) &#123; I.getNode()-&gt;visit(this, visitor); &#125; &#125; &#125; visitor-&gt;post(parent, this);&#125; 由于搞反了父子节点规定和root节点，笔者一直以为post order得到的顺序是，正常计算顺序的反序。例如，mnist网络计算图(图太大，放链接)计算的顺序是conv-pool2-relu311-conv1-…-fc_1X-fc_dot-fc_add_bias-sm-return这个计算图实际上是layer堆叠的，因此结构并不复杂。按照构造顺序，我们容易觉得conv是pool2的父节点；然而，按照一般的计算图的定义方式。最后的return节点才是父节点，即从底向上的计算。因此，PostOrder的遍历，其实恰是正常计算的顺序。 下面具体介绍一下Glow中的ChildMemSizeBasedScheduler。按照上面的定义，节点的输入是其计算图中的子节点。以Add节点为例，C = A + B。是先算ANode还是BNode呢，衡量指标是Metric[child] = maxMemSize_[child] - resultMemSize_[child]，较大者优先。 123456789101112131415// pesudo codevoid orderChildNodesAndSchedule(Node *N)&#123; 1. if N is Scheduled or N is Variable(weight input for op node, not activation) return; 2. for input of N children.push_back(input) 3. order children by Metric desn 4. for child in children orderChildNodesAndSchedule(child) 5. scheduled_.push_back(N)&#125; 其中节点N的MaxMemSize应该这样规定：max(sum(resultMemSize of children), max(maxMemSize of children))具体的数据结构和算法如下1234567891011121314151617181920212223242526272829/// Required number of bytes to hold the results of a given node.std::unordered_map&lt;const Node *, int64_t&gt; resultMemSize_;/// Max number of bytes required during the computation of a given node.std::unordered_map&lt;const Node *, int64_t&gt; maxMemSize_;void ChildMemSizeBasedScheduler::computeNodeComputationMaxMemorySize() &#123; // Traverse nodes in such a way, that dependnecies are processed // before the node using them. GraphPostOrderVisitor visitor(G_); for (auto *N : visitor.getPostOrder()) &#123; int64_t maxSize = (N-&gt;getNumInputs() &gt; 0) ? std::max(resultMemSize_[N-&gt;getNthInput(0)], maxMemSize_[N-&gt;getNthInput(0)]) : 0; for (size_t idx = 1, e = N-&gt;getNumInputs(); idx &lt; e; ++idx) &#123; const auto &amp;input = N-&gt;getNthInput(idx); // Skip operands that do not require memory allocations for storing // their results. if (isa&lt;Storage&gt;(input)) continue; assert(resultMemSize_.count(input) &gt; 0); assert(maxMemSize_.count(input) &gt; 0); maxSize += resultMemSize_[input]; if (maxSize &lt; maxMemSize_[input]) maxSize = maxMemSize_[input]; &#125; maxMemSize_[N] = maxSize; &#125;&#125; DeviceManager设计当我们考虑分布式执行时，需要将Devices管理起来，需要统一的接口，并考虑和计算图之间的交互问题，包括执行和代码生成等角度。 Variable拆分为Constants和Placeholder较早版本的glow用Variable表示作为Weight的容器，如input，Conv的filter和bias等。这些Weights又可以分为两类，在会变动的MutableWeights（输入，训练），和ConstantWeights（如Zero-Tensor和推理时的Weights）。Glow的实现是在IRGen时将所有Variable对应的WeightVar设置为Mutable，在IROptimizer运行时，将无需写的WeightVar属性设置为Constans。12345678910111213141516171819202122232425// IRGencase glow::Kinded::Kind::VariableKind: &#123; auto *V = cast&lt;Variable&gt;(N); auto *W = builder_.createWeightVar(V-&gt;getType(), V-&gt;getName(), WeightVar::MutabilityKind::Mutable, V-&gt;getVisibilityKind()); W-&gt;setName(N-&gt;getName()); registerIR(N, W); break;&#125;// IROptimizer// For each instruction that uses the weight:for (const auto &amp;U : ValueUses(W)) &#123; auto kind = U.getOperand().second; // Check if all of the users are read-only. if (kind != OperandKind::In) &#123; readOnly = false; break; &#125;&#125;// Mark the variable as read only.if (readOnly) W-&gt;setMutability(WeightVar::MutabilityKind::Constant); 备注：Varialb的Visibility属性，设置为Public(意味着可能hold a reference)时不可对其进行优化，且对应的WeightVar必须是Mutable的。 Variable的成员变量如下：1234567891011class Variable : public Node &#123; /// Specifies if the variable is trainable. bool isTrainable_; /// Specifies the visibility of the variable. VisibilityKind visibility_; /// The tensor payload that the variable holds. Tensor payload_; ...public: Tensor &amp;getPayload() &#123; return payload_; &#125;&#125;; Placeholder和Constant对比两个类的定义:123456789101112131415161718192021222324252627class Storage : public Node &#123;public: ... // some functions&#125;;class Constant : public Storage&#123; /// The tensor payload that the constant holds. Tensor payload_;public: ...&#125;;/// Placeholder nodes are unbound-storage. The content tensors are attached to/// this node at runtime. Placeholders are used as inputs and output nodes to/// the network.class Placeholder : public Storage &#123; /// Specifies if the placeholder is trainable. bool isTrainable_;public: /// Create a new placeholder. Placeholder(llvm::StringRef name, TypeRef Ty, bool isTrainable) : Storage(Kinded::Kind::PlaceholderKind, name), isTrainable_(isTrainable) &#123; addResult(Ty); &#125; ...&#125;; 在应用方面，以lenet mnist网络为例，构造网络时input是Placeholder，Conv的filte和bias同样声明为Placeholder。(注意Placeholder构造函数中的addResult(Ty))1234567891011121314// mnist.cppPlaceholder *A = mod.createPlaceholder( ElemKind::FloatTy, &#123;minibatchSize, 28, 28, 1&#125;, "input", false);...Tensor *inputTensor = ctx.allocate(A);// Context.cppTensor *Context::allocate(Placeholder *P) &#123; assert(!map_.count(P) &amp;&amp; "Placeholder already registered"); Tensor *T = new Tensor(P-&gt;getType()); map_[P] = T; nameMap_[P-&gt;getName()] = P; return T;&#125; ContextContext用来维护神经网络的什么信息呢？ mapping between some graph nodes and concrete tensors traceEvents owns the tensors 1)tensors是在成员函数allocate()中分配的；2)insert(Placeholder *P, Tensor &amp;&amp;T)使用了右值引用和std::move()。 123456789101112131415161718192021222324/// This class provides a mapping between some graph nodes, which are a symbolic/// representation of some computation, and concrete tensors that represent the/// inputs and outputs to the graph. The context owns the tensors and the graph/// uses these values as runtime. This is useful for the multi-threaded/// execution of code, where each thread has a different execution context. The/// difference between this class and a regular map is that the Context owns the/// Tensors (not only the pointers) and manages their lifetime.class Context final &#123;public: /// Maps placeholders to the tensors that back them. using PlaceholderMap = std::unordered_map&lt;Placeholder *, Tensor *&gt;; /// Maps Placeholder names to Placeholders. using PlaceholderNameMap = std::unordered_map&lt;std::string, Placeholder *&gt;;private: /// Maps Placeholders to Tensors. PlaceholderMap map_; /// Maps Placeholder names to Placeholders. PlaceholderNameMap nameMap_; /// Trace Events recorded during this run. std::vector&lt;TraceEvent&gt; traceEvents_; Context::insert()的使用实例如下：12345// ExecutorTest.cppauto refCtx = llvm::make_unique&lt;Context&gt;();auto *tensor = testCtx-&gt;allocate(placeholder.get());tensor-&gt;init(Tensor::InitKind::Xavier, 1.0, rng);refCtx-&gt;insert(placeholder.get(), tensor-&gt;clone()); Backendsclass Backend接口类，重要函数如下compile():virtual std::unique_ptr compile(Function *F) = 0; class BackendUsingGlowIR : public BackendcompileIR(): 增加了从IRFunction编译到CompiledFunction的接口compileIR()（在LLVMBackend中实现）virtual std::unique_ptr&lt;CompiledFunction&gt; compileIR(std::unique_ptr&lt;IRFunction&gt; IR) = 0; class LLVMBackend : public BackendUsingGlowIRcompile()的实现。compileIR()的实现。 createIRGen(): 构造LLVM IR Generator的接口。（具体的后端应该做平台相关的实现）virtual std::unique_ptr&lt;LLVMIRGen&gt; createIRGen(const IRFunction *IR, AllocationsInfo &amp;allocationsInfo) const = 0; createCompliedFunction(): 构造CompiledFunction的接口（在CPUBackend中实现）virtual std::unique_ptr&lt;CompiledFunction&gt; createCompiledFunction(std::unique_ptr&lt;llvm::orc::GlowJIT&gt; JIT, const runtime::RuntimeBundle &amp;runtimeBundle) const = 0; class CPUBackend : public LLVMBackendcreateIRGen()的实现。提供了对LLVMBackend中接口的实现。compile(Function ) generateAndOptimizeIR(Function ) compileIR(IRFunction *) createIRGen() glow::generateRuntimeBundle() createCompiledFunction() class CompiledFunction123456789101112// execute()是最为重要的接口，CompiledFunction中没有编译后的底层IR，需要其继承类来实现。// RuntimeBundle中保存了SymbolTable和内存大小信息(constant,// placeholder, activation, 独立的Allocator分配，故偏移量独立)；// 如果collectConstants，还有分配用来聚合的constants空间。class CompiledFunction &#123;public: virtual void execute(Context *ctx) = 0; ...protected: runtime::RuntimeBundle runtimeBundle_; ...&#125;; 存储管理CPUBackend123456789101112131415161718192021222324252627282930313233343536373839void LLVMCompiledFunction::execute(Context *ctx) &#123; uint8_t *baseActivationsAddress&#123;nullptr&#125;; /// Base address for Mutable weights memory block, Inputs and Outputs. uint8_t *baseMutableWeightVarsAddress&#123;nullptr&#125;; if (runtimeBundle_.getActivationsSize() != 0) &#123; baseActivationsAddress = (uint8_t *)alignedAlloc( runtimeBundle_.getActivationsSize(), TensorAlignment); &#125; if (runtimeBundle_.getMutableWeightSize() != 0) &#123; baseMutableWeightVarsAddress = (uint8_t *)alignedAlloc( runtimeBundle_.getMutableWeightSize(), TensorAlignment); &#125; loadPlaceholders(ctx, baseMutableWeightVarsAddress); auto sym = JIT_-&gt;findSymbol("jitmain"); assert(sym &amp;&amp; "Unable to JIT the code!"); using JitFuncType = void (*)(uint8_t * constantWeightVars, uint8_t * mutableWeightVars, uint8_t * activations); auto address = sym.getAddress(); if (address) &#123; JitFuncType funcPtr = reinterpret_cast&lt;JitFuncType&gt;(address.get()); funcPtr(runtimeBundle_.getConstants(), baseMutableWeightVarsAddress, baseActivationsAddress); &#125; else &#123; GLOW_UNREACHABLE("Error getting address"); &#125; updatePlaceholders(ctx, baseMutableWeightVarsAddress); alignedFree(baseMutableWeightVarsAddress); alignedFree(baseActivationsAddress); translateTraceEvents(ctx);&#125; Glow中间activations Tensor维度推导我们以Convolution为例，在ConvolutionNode *Function::createConv(...)函数中，计算输出维度并作为ConvolutionNode的构造函数参数。12345auto outSz = calculateConvPoolOutputDims(idim.h, idim.w, kernels, strides, pads);...auto OT = getParent()-&gt;uniqueType(ElemKind::FloatTy, outDims);return addNode(new ConvolutionNode(name, OT, input, filter, bias, kernels, strides, pads, group)); 其中uniqueType的返回类型是TypeRef.using TypeRef = const Type *;1TypeRef Module::uniqueType(ElemKind elemTy, llvm::ArrayRef&lt;size_t&gt; dims);]]></content>
      <tags>
        <tag>C++</tag>
        <tag>轮子</tag>
        <tag>Glow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++ 模板使用]]></title>
    <url>%2F2019%2F03%2F03%2Fcpp-template%2F</url>
    <content type="text"><![CDATA[函数模板缺省模板类型看代码。12auto LIH = labelInputs.getHandle&lt;int64_t&gt;();auto IIH = imageInputs.getHandle&lt;&gt;(); 其中labelInputs和imageInputs是Class Tensor的instances。函数声明如下。1234567/// \return a new handle that points and manages this tensor.template &lt;class ElemTy = float&gt; Handle&lt;ElemTy&gt; getHandle() &amp;;template &lt;class ElemTy = float&gt; const Handle&lt;ElemTy&gt; getHandle() const &amp;;/// If Tensor is rvalue, it is an error to get its Handle.template &lt;class ElemTy = float&gt; Handle&lt;ElemTy&gt; getHandle() &amp;&amp; = delete; 上面的示例设置了模板参数默认的类型。因此IIH的默认是float类型。此外，我们还可以了解到： 模板参数可以作为类型(函数返回类型或者参数类型)，类模板的模板参数(如上面代码的Handle&lt;ElemTy&gt;)。其中类模板Handle的声明为template &lt;class ElemTy&gt; class Handle final {...}; 模板类的显示实例化(explicit instantiation)和隐式实例化(implicit instantiation)以一个示例来说明。假定我们声明了函数模板1234template &lt;class T&gt;T GetMax (T a, T b) &#123; return (a&gt;b?a:b);&#125; 显式实例化我们可以像”重复声明“那样，告诉编译器要做的实例化工作。切记的是，实例化发生在编译期间。显式实例化就是主动告诉编译器如何实例化。123int x,y;GetMax&lt;int&gt;(int, int);GetMax &lt;int&gt; (x,y); 隐式实例化隐式实例化就是在函数调用处，编译时由编译器自己推断应该如何实例化。12int i,j;GetMax (i,j); 类模板类模板的显示和隐式实例化显式实例化可以这样123template &lt;class ElemTy&gt; class Handle final &#123;...&#125;;template class Handle&lt;float&gt;;template class Handle&lt;double&gt;;]]></content>
      <tags>
        <tag>C++</tag>
        <tag>模板</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zsh, oh-my-zsh 安装配置]]></title>
    <url>%2F2019%2F03%2F01%2Fzsh-install%2F</url>
    <content type="text"><![CDATA[1. ZshZsh，即Z shell。$cat /etc/shells可以查看所有的shells，如bash，tcsh，zsh等。如果已经有了zsh，则命令$chsh -s /bin/zsh即可切换到zsh shell。命令$echo $SHELL查看，则结果为/usr/bin/zsh或者/bin/zsh。关于二者的关系，以下命令可以说明。12$ls -ls /usr/bin/zsh... /usr/bin/zsh -&gt; /bin/zsh zsh 与 .bash_profile .bashrc已知.bashrc是每当bash “started interactively”时都会运行；.bash_profile仅仅“at the start of a new login shell”时执行。那么，使用zsh时.bashrc和.bash_profile就不会生效。实际上，zsh相关的文件如下$ZDOTDIR/.zshenv 类似.bash_profile$ZDOTDIR/.zprofile 类似.bash_profile$ZDOTDIR/.zshrc 类似.bashrc$ZDOTDIR/.zlogin$ZDOTDIR/.zlogout Installing ZSH 2. oh-my-zsh及插件配置 一般推荐的安装方式为sh -c &quot;$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)&quot;或者sh -c &quot;$(wget https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)&quot;给非root用户安装，或者需要指定安装位置，可以手动安装。 手动安装 git clone https://github.com/robbyrussell/oh-my-zsh.git ~/.oh-my-zsh 如果.zshrc中已经有了相关配置，可以将其备份 cp ~/.oh-my-zsh/templates/zshrc.zsh-template ~/.zshrc 安装 zsh-syntax-highlightingzsh-syntax-highlighting的作用是shell命令高亮。Mac OS X等系统安装brew install zsh-syntax-highlighting或其他对应的命令将source /usr/local/share/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh添加到~/.zshrc的最后。 手动安装则可以直接下载git repo。 git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting 在.zshrc的plugins选项中添加plugins=( [plugins…] zsh-syntax-highlighting) 重启zsh或者 source ~/.zshrc 安装 zsh-autosuggestion及iTerm2下不可用的解决办法利用手动安装的方式，即 git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions ~/.zshrc中设置plugins=(... zsh-autosuggestions) 发现未能生效，以为Mac下最好使用brew安装的方式 brew install zsh-autosuggestions ~/.zshrc中添加source /usr/local/share/zsh-autosuggestions/zsh-autosuggestions.zsh发现还是未能生效；但是，默认的bash可以生效;进一步测试发现iTerm2下按cmd+A命令时也会执行补全。最终参考这篇文章ZSH_AUTOSUGGEST_HIGHLIGHT_STYLE=&#39;fg=6&#39; Install oh-my-zshzsh-syntax-highlighting]]></content>
  </entry>
  <entry>
    <title><![CDATA[Reloading]]></title>
    <url>%2F2019%2F02%2F27%2Freloading%2F</url>
    <content type="text"><![CDATA[Reloading由于之前的（2015，真的是很久很久之前，之后便断掉了）post在旧电脑上直接保存，这次更新直接把原来的内容给覆盖了。 继续使用next主题，但是配置还要从头再来。做好记录，切实可以回看的记录，果然还是非常重要的。 Hexo使用使用资源文件在hexo的sources中建立相应的文件夹，如images/, files/等，在链接时使用[]()就可以了,图片则为![]()如，channel_v3.jsonzhonghan wu等人关于图计算的综述论文一篇Markdown在线教程(英文) Markdown使用markdown-image-pastectrl+V粘贴无效的问题。 问题来自插件中的代码错误。OSX系统下，View &gt; Developer &gt; Toggle Developer Tools打开console，执行时可以看到如下错误： 打开$HOME/.atom/packages/markdown-image-paste/lib/markdown-img-paste.coffee,将其中的toPng()改为toPNG()即可。 github issue 链接]]></content>
      <tags>
        <tag>general</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F02%2F27%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
